{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": false,
        "id": "_j55y1q8aA44",
        "inputHidden": false,
        "outputHidden": false
      },
      "source": [
        "\n",
        "# Scikit-learn model optimisation\n",
        "\n",
        "## Start from good old GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5vyzOQaIaA46",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "# by Andrey Ustyuzhanin with heavy scikit-learn documentation re-use\n",
        "# Kudos to Raghav RV <rvraghav93@gmail.com>\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.datasets import make_hastie_10_2, make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.manifold import locally_linear_embedding\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pXVoFICYaA48",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "# create X, y dataset for classification that has 2000 samples and 20 features with 10 informative features and 2 classes\n",
        "# random_state=0 to make the results reproducible\n",
        "\n",
        "X, y = make_classification(2000, 20, n_informative=10, n_classes=2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA embedding\n",
        "X_pca = PCA(n_components=2).fit_transform(X)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='bwr', alpha=0.5);\n",
        "X_pca.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_lle, err = locally_linear_embedding(X, n_neighbors=5, n_components=2)\n",
        "# PCA would be a better choice here, but LLE is more fun\n",
        "plt.scatter(X_lle[:, 0], X_lle[:, 1], c=y, cmap='bwr', alpha=0.5);\n",
        "X_lle.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "biHLC7-taA4-",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "# create dictionary with single key `min_samples_split` \n",
        "# that defines range from 2 to 402 with step 10\n",
        "param_grid = {'min_samples_split': np.arange(2, 402, 10)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The scorers can be either be one of the predefined metric strings or a scorer\n",
        "# callable, like the one returned by make_scorer\n",
        "# create dictionary of scorers that are used for grid search. \n",
        "# it should contain 'accuracy' and 'roc_auc' keys\n",
        "scorers = {'accuracy': make_scorer(accuracy_score), 'roc_auc': 'roc_auc'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run grid search with DecisionTreeClassifier, param_grid, scorers that computes training scores\n",
        "# and 5-fold cross-validation\n",
        "grid_obj = GridSearchCV(DecisionTreeClassifier(), param_grid, scoring=scorers, refit='accuracy', cv=5, return_train_score=True)\n",
        "grid_fit = grid_obj.fit(X, y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oEG73r_HehST"
      },
      "outputs": [],
      "source": [
        "# Let's examine keys available inside 'cv_results_' attribute of grid_fit\n",
        "grid_fit.cv_results_.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NzCd0GQ8aA5E",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "def plot_optimisation(results, scoring, param_name):\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "  plt.title(\"GridSearchCV evaluating\", fontsize=14)\n",
        "\n",
        "  plt.xlabel(param_name)\n",
        "  plt.ylabel(\"Score\")\n",
        "\n",
        "  ax = plt.axes()\n",
        "  y_min, y_max = 1e10, -1e10\n",
        "\n",
        "  # Get the regular numpy array from the MaskedArray\n",
        "  X_axis = np.array(results['param_' + param_name].data, dtype=float)\n",
        "\n",
        "  for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
        "      for sample, style in (('train', '--'), ('test', '-')):\n",
        "          sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
        "          sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
        "          ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
        "                          sample_score_mean + sample_score_std,\n",
        "                          alpha=0.1 if sample == 'test' else 0, color=color)\n",
        "          ax.plot(X_axis, sample_score_mean, style, color=color,\n",
        "                  alpha=1 if sample == 'test' else 0.7,\n",
        "                  label=\"%s (%s)\" % (scorer, sample))\n",
        "          y_max = max(np.max(sample_score_mean + 1.5 * sample_score_std), y_max)\n",
        "          y_min = min(np.min(sample_score_mean - 1.5 * sample_score_std), y_min)\n",
        "          \n",
        "\n",
        "      best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
        "      best_score = results['mean_test_%s' % scorer][best_index]\n",
        "\n",
        "      # Plot a dotted vertical line at the best score for that scorer marked by x\n",
        "      ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
        "              linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
        "\n",
        "      # Annotate the best score for that scorer\n",
        "      ax.annotate(\"%0.2f\" % best_score,\n",
        "                  (X_axis[best_index], best_score + 0.005))\n",
        "\n",
        "  ax.set_ylim(y_min, y_max)\n",
        "  plt.legend(loc=\"best\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BgzaPyoAbiAi"
      },
      "outputs": [],
      "source": [
        "# plot grid search results\n",
        "plot_optimisation(grid_fit.cv_results_, scorers, 'min_samples_split')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJmZF2Q0aA5G"
      },
      "source": [
        "## RandomizedSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ggXzc0fNaA5H",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import randint as sp_randint\n",
        "from time import time\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5cxI9m6Lrsls"
      },
      "outputs": [],
      "source": [
        "# get some data\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Let's check X and X[0] shapes\n",
        "print(<YOUR CODE>) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "j4DqWZLpaA5T",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "# Utility function to report best scores\n",
        "def report(results, n_top=3, score_name='roc_auc'):\n",
        "    rank_key = 'rank_test_%s' % score_name\n",
        "    mean_key = 'mean_test_%s' % score_name\n",
        "    std_key = 'std_test_%s' % score_name\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results[rank_key] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation {0}, score: {1:.3f} (std: {2:.3f})\".format(\n",
        "                    score_name,\n",
        "                    results[mean_key][candidate],\n",
        "                    results[std_key][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vc4HCA6AaA5Z",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "# build a classifier\n",
        "clf = RandomForestClassifier(n_estimators=20)\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {\"max_depth\": [3, None],\n",
        "              \"max_features\": sp_randint(1, 11),\n",
        "              \"min_samples_split\": sp_randint(2, 11),\n",
        "              \"min_samples_leaf\": sp_randint(1, 11),\n",
        "              \"bootstrap\": [True, False],\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uZvCs7EmaA5c",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "# run randomized search\n",
        "n_iter_search = 60\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring=scorers,\n",
        "                                   n_iter=n_iter_search, refit='accuracy', cv=5, return_train_score=True)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report(random_search.cv_results_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VLxt9b-8aA5h"
      },
      "source": [
        "### Homework. Compare with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "67xmkKaqaA5l",
        "inputHidden": false,
        "outputHidden": false
      },
      "outputs": [],
      "source": [
        "# use a full grid over all parameters\n",
        "param_grid = {\"max_depth\": [3, None],\n",
        "              \"max_features\": <YOUR CODE>, # logarithmic from 1 to 10\n",
        "              \"min_samples_split\": [2, 3, 10],\n",
        "              \"min_samples_leaf\": [1, 3, 10],\n",
        "              \"bootstrap\": <YOUR CODE>, # either True or False\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
        "start = time()\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "report(grid_search.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_VKJUPEKpz3r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1-scikit-search.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
