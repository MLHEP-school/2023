{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyro-ppl\n",
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "from pyro import distributions as dist\n",
    "from pyro import poutine\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "#from model import YModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "my_cmap = plt.cm.jet\n",
    "my_cmap.set_under('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To formulate an optimisation problem, one starts with the definition of the function to be minimised We define the objective function $R(\\psi)$ as a function that depends on the parameter vector $\\psi$. This function returns a number that in some way characterises the performance of the optimised object (e.g. the total flux of particles through the detector). Thus, the goal of the optimisation is to find a $\\psi^*$ that minimises the objective function:\n",
    "\n",
    "$$\n",
    "\\psi^{*} = argmin_{\\psi} R(\\psi)\n",
    "$$\n",
    "\n",
    "As an example, let the objective function be the 1-dimensional Higgs potential:\n",
    "\n",
    "\\begin{equation}\n",
    "    R(\\psi) = -5 \\psi^2 + \\psi^4\n",
    "\\end{equation}\n",
    "\n",
    "In this simple scenario, one can find the minimum analytically. There are two degenerate minima at $\\psi^* = \\pm \\sqrt{5/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hat(x):\n",
    "    return -5 * x**2 + x**4\n",
    "\n",
    "def hat_derivative(x):\n",
    "    return -10 * x + 4 * x** 3\n",
    "\n",
    "def hat_numpy(x):\n",
    "    return (-5 * np.array(x)**2 + np.array(x)**4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "rang = np.linspace(-3, 3, 100)\n",
    "plt.plot(rang, hat(rang))\n",
    "\n",
    "plt.xlabel(r\"$y$\", fontsize=19)\n",
    "plt.ylabel(r\"$\\mathcal{R}(y)$\",fontsize=19)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, such a simple function does not resemble what one actually observes in a simulation or experiment. For example, in an experiment there will always be some noise observed. In a simulator, one often does not even know the exact dependency of the loss on the inputs.\n",
    "\n",
    "For instance, the parameters $\\psi$ may be treated as unknown, yet affect the value of the observables $y$. The observable $y$ may be a random variable, sampled from a normal distribution with mean equal to $\\psi$ and fixed standard deviation $\\sigma$:  $y \\sim N(y; \\psi, \\sigma)$. Now, the objective function $R$ depends on $\\psi$ implicitly: the only variable that we actually observe is $y$:\n",
    "\n",
    "\\begin{equation}\n",
    "    R(y) = -5 y^2 + y^4,\\quad \\text{s.t.}  \\ \\ y \\sim \\mathscr(y; \\psi, \\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Such formulation results in a distribution over values of $R$ as a function of $\\psi$, rather than an exact value, as in the initial example. The corresponding plot of the mean and the standard deviation of $R(\\psi)$ is shown in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this exercise, we are going to use Pyro package, a powerful package for probabilistic programming. It allows for automatic differentiation through various probabilistic models and we are going to use this property to optimise our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = torch.linspace(-3, 3, 1000)\n",
    "NOISE = 0.2\n",
    "\n",
    "def sample_y(psi):\n",
    "    \"\"\"\n",
    "    We define y \\sim N(psi, \\sigma) as above.\n",
    "    \"\"\"\n",
    "    y_dist = dist.Normal(psi, NOISE)\n",
    "    y = pyro.sample('y', y_dist)\n",
    "    return hat(y)\n",
    "\n",
    "def sample_y_numpy(psi):\n",
    "    \"\"\"\n",
    "    Same as above, but returns numpy\n",
    "    \"\"\"\n",
    "    if isinstance(psi, float) or isinstance(psi, int):\n",
    "        psi = [psi]\n",
    "    y_dist = dist.Normal(torch.Tensor(psi), NOISE)\n",
    "    y = pyro.sample('y', y_dist)\n",
    "    return hat(y.numpy())\n",
    "\n",
    "def sample_y_derivative_numpy(psi):\n",
    "    if isinstance(psi, float) or isinstance(psi, int):\n",
    "        psi = [psi]\n",
    "    y_dist = dist.Normal(torch.Tensor(psi), NOISE)\n",
    "    y = pyro.sample('y', y_dist)\n",
    "    return hat_derivative(y.numpy())\n",
    "\n",
    "def sample_single_value(psi):\n",
    "    return sample_y_numpy(psi)[0]\n",
    "    \n",
    "\n",
    "y = sample_y(psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $R$ is a random variable now, it is natural to reformulate the optimisation problem in terms of the expected value of the function: \n",
    "\n",
    "\\begin{equation}\n",
    "     \\psi^{*} = argmin_{\\psi} E_{p(y;\\psi, \\sigma)}[R(y)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see how the actual noisy function will look like. Because it is stochastic now, we will plot the mean value of $R$ and $1 \\sigma$ interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = []\n",
    "means = []\n",
    "_range = torch.linspace(-3, 3, 100)\n",
    "for psi in _range:\n",
    "    std = sample_y(psi.repeat([1000])).std()\n",
    "    stds.append(std.item())\n",
    "    mean = sample_y(psi.repeat([1000])).mean()\n",
    "    means.append(mean.item())\n",
    "stds = np.array(stds)\n",
    "means = np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(_range, means, '--', c='r', linewidth=2, label=\"Mean value\")\n",
    "plt.fill_between(_range, means - stds, means+stds, alpha=0.2, color='green', label=r\"$1\\sigma$\")\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel(r\"$\\psi$\", fontsize=19)\n",
    "plt.ylabel(r\"$\\mathcal{R}(\\psi)$\",fontsize=19)\n",
    "plt.gca().tick_params(axis='both', labelsize= 15)\n",
    "plt.gca().tick_params(axis='both', labelsize= 15)\n",
    "\n",
    "\n",
    "plt.ylim(-7,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true function.\n",
    "def plot_base():\n",
    "    plt.plot(_range, hat(_range), '--', c='r', linewidth=2, label=\"True function\")\n",
    "\n",
    "\n",
    "    plt.grid()\n",
    "    #plt.legend()\n",
    "    plt.xlabel(r\"$\\psi$\", fontsize=15)\n",
    "    plt.ylabel(r\"$\\mathcal{R}(\\psi)$\",fontsize=15) \n",
    "    plt.ylim(-7,10)\n",
    "\n",
    "def plot_grads(x_history, func_history, derivatives, index):\n",
    "    b = func_history[index] - derivatives[index] * x_history[index]\n",
    "    \n",
    "    dx=0.2\n",
    "    plt.arrow(x_history[index] + dx,\n",
    "              derivatives[index] * (x_history[index] + dx) + b,\n",
    "              dx/4, derivatives[index] * dx/4, zorder=10, width=0.03, color='g')\n",
    "    plt.plot([x_history[index], x_history[index] + dx], [derivatives[index] * x_history[index] + b,\n",
    "                        derivatives[index] * (x_history[index] + dx) + b], '-', linewidth=1.5, label='Derivative', c='g')    \n",
    "    \n",
    "def plot_history(func_history):\n",
    "    plt.plot(range(len(func_history)), func_history)\n",
    "    plt.grid()\n",
    "    plt.xlabel(r\"iteration\", fontsize=15)\n",
    "    plt.ylabel(r\"$\\mathcal{R}(\\psi)$\",fontsize=15)\n",
    "#     plt.gca().tick_params(axis='both', labelsize= 15)\n",
    "#     plt.gca().tick_params(axis='both', labelsize= 15)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tractable derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, because we are using Pyro and because this this a toy problem, we actually have access to our gradeints!\n",
    "So we can perform optimisation using PyTroch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_pyro(func, x_init, n_iter=15, alpha=0.1):\n",
    "    x_history = []\n",
    "    func_vals = []\n",
    "    derivatives = []    \n",
    "    x = torch.tensor([x_init], requires_grad=True)\n",
    "    \n",
    "    for index in range(n_iter):\n",
    "        func_value = func(x)\n",
    "        func_value.backward()\n",
    "        x_new = x - alpha * x.grad.data\n",
    "        derivatives.append(x.grad.data.item())\n",
    "        x_history.append(x.item())\n",
    "        func_vals.append(func_value.item())\n",
    "        x.data = x_new\n",
    "    return x_history, func_vals, derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 40\n",
    "x_history, func_history, derivatives = SGD_pyro(sample_y, 0., alpha=0.005, n_iter=N_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 4\n",
    "n_plots = N_ITER // step\n",
    "\n",
    "for index in range(1, N_ITER+1, step):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plot_base()\n",
    "    plt.scatter(x_history[:index], func_history[:index], label='Evaluated points')\n",
    "    plot_grads(x_history, func_history, derivatives, index-1)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in reality, we would not be able to use Pyro to compute gradients. Moreover, we will not be able to compute derivative analytically.\n",
    "\n",
    "You should write a function that returns a numerical derivative of any input, given the step $h$:\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac{f(x+h) - f(x-h)}{2h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_derivative(func, x, step):\n",
    "    # Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use that function to perforom SGD optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(func, x_init, n_iter=15, alpha=0.06, step=0.1):\n",
    "    x_history = [x_init]\n",
    "    func_vals = [func(x_init)]\n",
    "    derivatives = []\n",
    "    x = x_init\n",
    "    \n",
    "    for index in range(n_iter):\n",
    "        derivative = # Your code here\n",
    "        derivatives.append(derivative)\n",
    "\n",
    "        x_new = # Your code here.\n",
    "        x_history.append(x_new)\n",
    "        func_vals.append(func(x_new))\n",
    "        x = x_new\n",
    "    return x_history, func_vals, derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 20 # your number\n",
    "x_history, func_history, derivatives = SGD(sample_y, 0, alpha=0.05, n_iter=N_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 4\n",
    "n_plots = N_ITER // step\n",
    "\n",
    "for index in range(1, N_ITER+1, step):\n",
    "    #plt.subplot(n_plots+1, 1, index)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plot_base()\n",
    "    plt.scatter(x_history[:index], func_history[:index], label='Evaluated points')\n",
    "    plot_grads(x_history, func_history, derivatives, index-1)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plot_history(func_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to use scikit optimisation package (https://scikit-optimize.github.io/stable/).\n",
    "It has a number of optimisation methods for Bayesian optimisation, and bayesian parameter grid search. It is good for quick prototyping but is quite slow and not actively supported.\n",
    "\n",
    "We are going to look at the BO with Gaussian processes, and visualise convergence plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence, plot_gaussian_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER=20\n",
    "res = gp_minimize(sample_single_value,                  # the function to minimize\n",
    "                  [(-50.0, 100.0)],      # the bounds on each dimension of x\n",
    "                  acq_func=\"EI\",      # the acquisition function\n",
    "                  n_calls=N_ITER,         # the number of evaluations of f\n",
    "                  n_initial_points=1, # the number of random initialization points\n",
    "                  n_restarts_optimizer=1,\n",
    "                  noise=0.01**2,       # the noise level (optional)\n",
    "                  random_state=1234) \n",
    "plot_convergence(res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x, res.fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you observe? Was the algorithm able to attain minima? If not, why?\n",
    "\n",
    "Look at the parameters of our optimiser function and think which of them seems to affect the results the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.rcParams[\"figure.figsize\"] = (8, 14)\n",
    "for index, n_iter in enumerate([1, 5, 14]):\n",
    "    # Plot true function.\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    if n_iter == 0:\n",
    "        show_legend = True\n",
    "    else:\n",
    "        show_legend = False\n",
    "    show_legend = False\n",
    "    ax = plot_gaussian_process(res, n_calls=n_iter,\n",
    "                               objective=hat_numpy,\n",
    "                               noise_level=0,\n",
    "                               show_legend=show_legend, show_title=False,\n",
    "                               show_next_point=False, show_acq_func=False)\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=15)\n",
    "    plt.xlabel(r\"$\\psi$\", fontsize=19)\n",
    "    plt.ylabel(r\"$\\mathcal{R}(\\psi)$\",fontsize=19)\n",
    "    plt.gca().tick_params(axis='both', labelsize= 15)\n",
    "    plt.gca().tick_params(axis='both', labelsize= 15)    \n",
    "    \n",
    "    # Plot EI(x)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ax = plot_gaussian_process(res, n_calls=n_iter,\n",
    "                               show_legend=show_legend, show_title=False,\n",
    "                               show_mu=False, show_acq_func=True,\n",
    "                               show_observations=False,\n",
    "                               show_next_point=True)\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.xlabel(r\"$\\psi$\", fontsize=19)\n",
    "    plt.ylabel(r\"EI $(\\psi)$\",fontsize=19)\n",
    "    plt.gca().tick_params(axis='both', labelsize= 15)\n",
    "    plt.gca().tick_params(axis='both', labelsize= 15)    \n",
    "    plt.savefig(\"BO_{}.pdf\".format(index), bbox_inches='tight', dpi=100)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolutionary algorithms are inspired by an evlution of spicies in biology. Usually, they iterate between sampling new points, evaluating functions, changing (\"mutating\") the obtained points and evaluating the function again.\n",
    "\n",
    "Now implement the simlpliest Gaussian evolutionary algorithm discussed during the lecture:\n",
    "\n",
    "- Sample $N$ $\\psi$ points from any distribution.\n",
    "- Evaluate our function\n",
    "- Select the best $K$ points\n",
    "- Update the parameters of the distribution\n",
    "\n",
    "Repate for **N_ITER** number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolutionary_algorithm(func, N=10, K=3, mean=0):\n",
    "    best_psi = []\n",
    "    best_func = []\n",
    "    for i in range(N_ITER):\n",
    "        samples = # Your code\n",
    "        func_samples = sample_y_numpy(samples)\n",
    "        sorted_ind = # Your code\n",
    "        elite = # Your code\n",
    "        # Update parameters of the distributions (mean)\n",
    "        \n",
    "        if not best_func or func_samples[sorted_ind][0] < best_func[-1]:\n",
    "            best_psi.append(samples[0])\n",
    "            best_func.append(func_samples[sorted_ind][0])\n",
    "        else:\n",
    "            best_psi.append(best_psi[-1])\n",
    "            best_func.append(best_func[-1])\n",
    "        \n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            plt.figure(figsize=(10,4))\n",
    "            plot_base()\n",
    "            #plt.grid()\n",
    "            plt.scatter(samples, func_samples, c='black', s=25)\n",
    "            plt.scatter(elite, func_samples[sorted_ind], c='b', s=25)\n",
    "            plt.title(f\"Iteration: {i}\")\n",
    "    return best_psi, best_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_psi, best_func = evolutionary_algorithm(sample_y_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(best_func)), best_func, \".-\", markersize=10)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Function value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE gradient estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we would like to explore the REINFORCE approach, discussed at the lecture. To do that, we will introduce a distribution over parameters $\\psi$. Lets assume that\n",
    "\n",
    "$$\n",
    "\\psi \\sim N(\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "but we will only be optimising w.r.t to $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, that when we use the REINFORCE approach, the gradient estimator for the objective becomes:\n",
    "\n",
    "$$\n",
    "      \\nabla_{\\psi} E_{p(y;\\psi, \\sigma)}[R(y)] \\approx \\nabla_{\\psi} E_{p(\\psi;\\mu, \\sigma)}[R(y)]\n",
    "      = E_{p(\\psi;\\mu, \\sigma)}[R(y) \\nabla_{\\mu} log p(\\psi;\\mu,\\sigma)]\n",
    "$$\n",
    "\n",
    "Compute the derivative: $\\nabla_{\\mu} log p(\\psi;\\mu, \\sigma)$ = ?\n",
    "\n",
    "Once we have the derivative, lets write an SGD loop, using the REINFORCE estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reinforce_grad(func, mu, n_samples=1, scale=1.):\n",
    "    \"\"\"\n",
    "    func: optimised function\n",
    "    mu: paramter w.r.t which we take gradients\n",
    "    n_samples: number of samples to estimate expectation\n",
    "    scael: scale of normal distribution of psi\n",
    "    \"\"\"\n",
    "    # Sample psi from Normal\n",
    "    # compute func value at sampled points\n",
    "    # compute grad log p(\\psi) at sampled point\n",
    "    # compute REINFORCE estimate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_reinforce(func, x_init, n_iter, alpha=0.06, n_samples=1, scale=1.):\n",
    "    x_history = [x_init]\n",
    "    func_vals = [func(x_init)]\n",
    "    derivatives = []\n",
    "    x = x_init\n",
    "    \n",
    "    for index in range(n_iter):\n",
    "        derivatives.append(compute_reinforce_grad(func, x, n_samples=n_samples, scale=scale))\n",
    "        x_new = x - alpha * derivatives[-1]\n",
    "        x_history.append(x_new)\n",
    "        func_vals.append(func(x_new))\n",
    "        x = x_new\n",
    "    return x_history, func_vals, derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER=30\n",
    "x_history, func_history, derivatives = SGD_reinforce(sample_y_numpy, 0, n_iter=N_ITER, alpha=0.03, n_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 4\n",
    "n_plots = N_ITER // step\n",
    "\n",
    "for index in range(1, N_ITER+1, step):\n",
    "    #plt.subplot(n_plots+1, 1, index)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plot_base()\n",
    "    plt.scatter(x_history[:index], func_history[:index], label='Evaluated points')\n",
    "    plot_grads(x_history, func_history, derivatives, index-1)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plot_history(func_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? If not, which parameters we might want to change to make the algorithm converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take home message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a plethora of algorithms available for black-box optimisation. All of them may be applied to the same problem. All of them might provide satisfactory results, but the quality of the results and the speed on convergence might differ orders of magnitude. So it is impoortant to choose the algorithm according to the problem at hand.\n",
    "\n",
    "For example, if the problem is high dimensional, it might make sense to start with the application of BO and after some number of iterations switch to gradient-based methods.\n",
    "If nothing works, think about where is the bottleneck of the underlying problem and if the existing algorithms can be modified to bypass the bottleneck.\n",
    "\n",
    "It is also crucial to understand the underlying assumptions of each optimisation algorithm and choose the hyperparameters according to the task. For example, in BO one wants to specify the search space so that the hypothetical minima would be located near the centre of the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
