{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "collapsed": false,
    "id": "I-46qxfQVf3-",
    "outputId": "e4fdd276-2138-4c2d-c868-8501ba07d652"
   },
   "outputs": [
   ],
   "source": [
    "!pip install shap catboost scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "NYJlklPEVadC"
   },
   "outputs": [
   ],
   "source": [
    "import urllib\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize # if this line fails, try restarting the kernel\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load imagenet class names\n",
    "import requests\n",
    "response = requests.get('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json')\n",
    "class_labels = {int(key): value for key, (code, value) in response.json().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "oT_nqMNgVads"
   },
   "source": [
    "### Step I: manual explanation with smoothgrad\n",
    "\n",
    "Let us begin by implementing our own little explainer for [DenseNet121](https://arxiv.org/abs/1608.06993) pretrained on ImageNet.\n",
    "\n",
    "For the sake of simplicity, we're gonna rely on [SmoothGrad](https://arxiv.org/pdf/1706.03825.pdf) explainer - a simple average of gradients over noisy inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "6ecf31de6b724be180cedfc2339d2f34",
      "e2840af0fa1b4c36955ba61b9ba75afc",
      "5eb55db55852431fb2e92d847eff340c",
      "4572fbd1cab94d9c811c50c6c45cbe53",
      "4e7a0ddbd355498f993deec2c4251671",
      "770c9bee5eb0454782eee394bc93444b",
      "ce2dfb336b8f44458c7cd0ed7c57e210",
      "5749fa72f1ea46c5bdbb7d6a9a765810"
     ]
    },
    "collapsed": false,
    "id": "9xNNT6nxVadv",
    "outputId": "fbc05344-4d7d-4555-87b3-919fb1d32586"
   },
   "outputs": [
   ],
   "source": [
    "# model = torchvision.models.densenet121(pretrained=True).train(False).to(device)\n",
    "model = torchvision.models.resnet50(pretrained=False).train(False).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/user/share/resnet50-0676ba61.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "collapsed": false,
    "id": "f72e7P5CVaeK",
    "outputId": "671afc10-2150-4fd5-db7a-e10f7f783e9b"
   },
   "outputs": [
   ],
   "source": [
    "!wget -q https://cdn2.adrianflux.co.uk/wp-fluxposure/uploads/2014/08/no-7.jpg -O img.jpg\n",
    "\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2)\n",
    "    probs = torch.softmax(model(image_tensor), dim=-1)[0]\n",
    "    \n",
    "for i, class_ix in enumerate(probs.argsort(descending=True)[:10]):\n",
    "    print(f\"#{i + 1}: p={probs[class_ix].item():.3f}\\t{class_labels[class_ix.item()]} ({class_ix.item()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "F1sZqGy3Vaee"
   },
   "source": [
    "Now let's implement SmoothGrad itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "collapsed": false,
    "id": "bLQPUjDTVaeg",
    "outputId": "22b6c8d2-b5f1-4cb7-f8d0-6791dc0f79d7"
   },
   "outputs": [
   ],
   "source": [
    "def explain_smoothgrad(image, class_ix, num_runs=32, sigma=0.1):\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2).requires_grad_(True)\n",
    "    \n",
    "    # repeat image tensor several times, add different noise to each copy\n",
    "    repeated_image = image_tensor.repeat(num_runs, 1, 1, 1)\n",
    "    noisy_images = repeated_image + sigma * torch.randn_like(repeated_image)\n",
    "    \n",
    "    class_scores = model(noisy_images)[:, class_ix]\n",
    "    class_scores.sum().backward()  # backpropagate to image_tensor\n",
    "    return abs(image_tensor.grad).mean(dim=1)[0].cpu().numpy()\n",
    "\n",
    "for class_ix in 163, 717, 897:  # <-- insert your classes here, use numbers in (brackets)\n",
    "    plt.title(f'Explaining \"{class_labels[class_ix]}\"')\n",
    "    plt.imshow(explain_smoothgrad(image, class_ix), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_GJSi9fNVaex"
   },
   "source": [
    "__Bonus round!__ Peter Higgs nobel prize photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "id": "PDrt9ZdEVae0",
    "outputId": "6f455a6e-ae4c-41f4-be36-5c6eeb1c4609"
   },
   "outputs": [
   ],
   "source": [
    "!wget -q https://i.pinimg.com/originals/3a/e1/83/3ae18369ab2e86be83e637ad702ec832.jpg -O img.jpg\n",
    "\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2)\n",
    "    probs = torch.softmax(model(image_tensor), dim=-1)[0]\n",
    "    \n",
    "for i, class_ix in enumerate(probs.argsort(descending=True)[:10]):\n",
    "    print(f\"#{i + 1}: p={probs[class_ix].item():.3f}\\t{class_labels[class_ix.item()]} ({class_ix.item()})\")\n",
    "    \n",
    "for class_ix in 834, 457, 861:\n",
    "    plt.title(f'Explaining \"{class_labels[class_ix]}\"')\n",
    "    plt.imshow(explain_smoothgrad(image, class_ix), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ynUzOdrfVafR"
   },
   "source": [
    "### Part II: SHapley Additive exPlanations\n",
    "Now, let's try something heavier. The current state of the art in explaining model predictions is [SHAP](https://arxiv.org/abs/1705.07874): Shapley Additive Explanations.\n",
    "\n",
    "This method is based on [Shapley values](https://en.wikipedia.org/wiki/Shapley_value) - a game-theoretic concept that evaluates the contribution of individual players in a cooperative game. Except this time our \"players\" are input features and the \"game\" is predicting whichever output the model gave.\n",
    "\n",
    "Computing Shapley values naively requires $O(F!)$ time where F is the number of features. To make this computation more feasible, authors [proposed](https://arxiv.org/abs/1705.07874) several approximations, one of which relies on averaged gradients. This approximation also requires \"background\" data - other images similar to the ones in question that can be used as reference points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 43
    },
    "collapsed": false,
    "id": "SSLAHAIrVafT",
    "outputId": "6736646b-a451-4cec-864e-50950ce545b9"
   },
   "outputs": [
   ],
   "source": [
    "import shap # pip install shap\n",
    "shap.initjs()\n",
    "\n",
    "# load \"background\" images - some 50 random images from ImageNet\n",
    "background, _ = shap.datasets.imagenet50()\n",
    "background = torch.as_tensor(background / 255.0, device=device, dtype=torch.float32)\n",
    "background = background.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "id": "ChXzVcT7Vafm"
   },
   "outputs": [
   ],
   "source": [
    "# image to explain\n",
    "!wget -q https://i.pinimg.com/originals/32/da/5c/32da5c3314fcc5ebf1a7b7d1548fcb03.jpg -O img.jpg\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "image_tensor = torch.as_tensor(image[None], device=device, dtype=torch.float32).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "collapsed": false,
    "id": "bngMHr8oVaf-",
    "outputId": "0209e27e-f231-438e-a1f8-6af1ea7bd3fc",
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# explain and visualize. If you're not using GPU, this may take a minute or two.\n",
    "explainer = shap.GradientExplainer((model, model.layer1), background)\n",
    "shap_values, indices = explainer.shap_values(image_tensor, ranked_outputs=5, nsamples=200)\n",
    "shap_values = [np.transpose(values, (0, 2, 3, 1)) for values in shap_values]\n",
    "index_names = np.vectorize(lambda i: class_labels[i])(indices.cpu().numpy())\n",
    "shap.image_plot(shap_values, image_tensor.permute(0, 2, 3, 1).cpu().numpy(), index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "collapsed": false,
    "id": "9k9n8fCvVagT",
    "outputId": "6dbaa89d-636c-4b2c-8a30-6f1ad01fb4a9"
   },
   "outputs": [
   ],
   "source": [
    "# obligatory physicist reference\n",
    "!wget -q https://images-na.ssl-images-amazon.com/images/I/51ArQaCkkZL._AC_.jpg -O img.jpg\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "image_tensor = torch.as_tensor(image[None], device=device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "explainer = shap.GradientExplainer((model, model.layer1), background)\n",
    "shap_values, indices = explainer.shap_values(image_tensor, ranked_outputs=3, nsamples=200)\n",
    "shap_values = [np.transpose(values, (0, 2, 3, 1)) for values in shap_values]\n",
    "index_names = np.vectorize(lambda i: class_labels[i])(indices.cpu().numpy())\n",
    "shap.image_plot(shap_values, image_tensor.permute(0, 2, 3, 1).cpu().numpy(), index_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fFtW7eDSVagl"
   },
   "source": [
    "### Part III: explaining classical machine learning models\n",
    "\n",
    "Finally, let's see how SHAP explainers can be applied to more conventional machine learning models like gradient boosting. \n",
    "\n",
    "Spoiler: exactly the same from a user's perspective. However, this time we're gonna use a different Shapley approximation implemented in TreeExplainer. For a full set of available explainers, take a look at their official [examples page](https://github.com/slundberg/shap/tree/master/notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "collapsed": false,
    "id": "ckkwFeh0Vago",
    "outputId": "0c70ab8f-9007-492d-e6a6-0ad17aa5e46d",
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "import catboost # pip install catboost\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "X, y = shap.datasets.boston()\n",
    "ensemble = catboost.CatBoostRegressor(iterations=100, learning_rate=0.1)\n",
    "ensemble.fit(X, y, verbose=False, plot=False)\n",
    "explainer = shap.TreeExplainer(ensemble)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# visualize explanation of the first example\n",
    "# (note: this visualization only works in regular jupyter!)\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "collapsed": false,
    "id": "o5eEu0VuVahA",
    "outputId": "bc2414ed-e4ba-4979-a529-fa283dd2a8f2"
   },
   "outputs": [
   ],
   "source": [
    "# Explain training data: each *column* is a rotated plot from above, stacked for all training samples\n",
    "# (this plot is interactive, hover mouse to see feature names)\n",
    "# (note: this visualization only works in regular jupyter!)\n",
    "shap.initjs()\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "h5JNc5cFXUSS"
   },
   "source": [
    "## References:\n",
    "* SHAP explainer based on superpixels - [notebook](https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html)\n",
    "* SHAP reference notebook - [view on github](https://github.com/slundberg/shap/tree/master/notebooks)\n",
    "* More various explainers in [ELI5](https://github.com/TeamHG-Memex/eli5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
   ],
   "name": "interpretability_demo.ipynb",
   "provenance": [
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4572fbd1cab94d9c811c50c6c45cbe53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5749fa72f1ea46c5bdbb7d6a9a765810",
      "placeholder": "​",
      "style": "IPY_MODEL_ce2dfb336b8f44458c7cd0ed7c57e210",
      "value": " 97.8M/97.8M [00:12&lt;00:00, 8.04MB/s]"
     }
    },
    "4e7a0ddbd355498f993deec2c4251671": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5749fa72f1ea46c5bdbb7d6a9a765810": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eb55db55852431fb2e92d847eff340c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_770c9bee5eb0454782eee394bc93444b",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e7a0ddbd355498f993deec2c4251671",
      "value": 102502400
     }
    },
    "6ecf31de6b724be180cedfc2339d2f34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5eb55db55852431fb2e92d847eff340c",
       "IPY_MODEL_4572fbd1cab94d9c811c50c6c45cbe53"
      ],
      "layout": "IPY_MODEL_e2840af0fa1b4c36955ba61b9ba75afc"
     }
    },
    "770c9bee5eb0454782eee394bc93444b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce2dfb336b8f44458c7cd0ed7c57e210": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2840af0fa1b4c36955ba61b9ba75afc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}