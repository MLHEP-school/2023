{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LNN-seminar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lagrangian Neural Networks (LNNs)"
      ],
      "metadata": {
        "id": "SKtjq0psLxN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook borrows heavily from the original [LNN tutorial](https://colab.research.google.com/drive/1CSy-xfrnTX28p1difoTA8ulYw0zytJkq) provided by the authors of the [LNN paper](https://arxiv.org/abs/2003.04630).*"
      ],
      "metadata": {
        "id": "gn2FJ-iSL0Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Lagrangian dynamics is a reformulation of the Newtonian dynamics using the Lagrangian function defined as:\n",
        "\n",
        "$$L(q,\\dot q, t)\\equiv T(q,\\dot q, t)-V(q,\\dot q, t),$$\n",
        "\n",
        "where $T$ is the kinetic energy of the system, $V$ is its potential energy, $q=(q_1, q_2,\\ldots, q_n)$ is a set of generalized coordinates describing the system and $\\dot q\\equiv\\frac{dq}{dt}$. Given a Lagrangian, the dynamics of the system is defined by the Euler-Lagrange (E-L) equations:\n",
        "\n",
        "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot q}=\\frac{\\partial L}{\\partial q}.$$\n",
        "\n",
        "As derived in the lecture material, it follows from the E-L equations that the accelerateion $\\ddot q$ is:\n",
        "\n",
        "$$\\ddot q = (\\nabla_{\\dot q}\\nabla_{\\dot q}^\\top L)^{-1}\\left[\\nabla_q L - (\\nabla_{\\dot q}\\nabla_q^\\top L)\\dot q\\right].$$\n",
        "\n",
        "This allows us to predict the evolution of the given system by integrating this equation numerically:\n",
        "\n",
        "$$\\frac{d}{dt}\n",
        "\\begin{pmatrix}\n",
        "q \\\\\n",
        "\\dot q\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\dot q \\\\\n",
        "\\ddot q\n",
        "\\end{pmatrix}.$$"
      ],
      "metadata": {
        "id": "Jxg3Sr7oNOlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The double pendulum problem"
      ],
      "metadata": {
        "id": "sQSh8p_wTOsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "NQEDS_BDTVLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll try to solve the double pendulum problem, as shown in the figure above. The system is formed by attaching one pendulum directly to another one. The state of the system is defined by the two angles: $q=(\\theta_1,\\theta_2)$. Its dynamics also depends on the values of the following static parameters:\n",
        " - $m_1$ and $m_2$ — masses of the pendulums,\n",
        " - $l_1$ and $l_2$ — their lengths,\n",
        " - $g$ — Earth's gravity acceleration."
      ],
      "metadata": {
        "id": "md_OQ5H_TcYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll use the analytical solution to this problem to generate the \"observed\" trajectories of this system. Our task will be to learn to predict the acceleration of the system $\\ddot q$ given it's current state $(q,\\dot q)$. We'll address it in two ways:\n",
        " 1. Assume we know the general form of the Lagrangian, but don't know the values of the parameters $(m_1, m_2, l_1, l_2, g)$. Here, we'll initialize these parameters randomly, make single step evolution predictions by integrating the E-L equations, and then calculate the prediction error and back-propagate it to the parameters to update their values by gradient descent.\n",
        " 2. Assume we don't know the general form of the Lagrangian — we'll parameterize it with a neural network and try to learn its parameters."
      ],
      "metadata": {
        "id": "g3yJhPnpVp-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since both approaches require a differentiable ODE integrator, we'll use [torchdiffeq](https://github.com/rtqichen/torchdiffeq/), which is a pytorch implementation of various ODE solvers. Let's install it:"
      ],
      "metadata": {
        "id": "QPpa0fH6X1TN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kIDuRCrP_sF"
      },
      "outputs": [],
      "source": [
        "!pip install torchdiffeq\n",
        "!pip install pytorch-lightning==1.9.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And make the necessary imports:"
      ],
      "metadata": {
        "id": "41ig_4zNYMMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchdiffeq\n",
        "from functools import partial # this allows to plug some of the inputs\n",
        "                              # of a function and return the function of\n",
        "                              # the rest of the inputs, see the docs:\n",
        "                              # https://docs.python.org/3.8/library/functools.html#functools.partial\n",
        "import numpy as np\n",
        "\n",
        "torch.set_default_dtype(torch.float64) # we'll work with 64 bit float precision"
      ],
      "metadata": {
        "id": "AH_ZsrbE5cjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning\n",
        "                            # installed by default.\n",
        "                            # Hence, we do it here if necessary\n",
        "    !pip install pytorch-lightning==1.3.4\n",
        "    import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "Y11w-ubjYhwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import trange\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation   # These two imports will be used to\n",
        "from IPython.display import HTML           # compile animations of the system\n",
        "                                           # dynamics."
      ],
      "metadata": {
        "id": "egO3YC_dCs7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the true parameters of our system in the following dictionary:"
      ],
      "metadata": {
        "id": "3BgqJGJlZNUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_parameters = dict(\n",
        "    m1=1, m2=1, l1=1, l2=1, g=9.8\n",
        ")"
      ],
      "metadata": {
        "id": "pcUswCv6QH9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The true Lagrangian is easy to derive:\n",
        "\n",
        "$$\n",
        "L = T_1+T_2-(V_1+V_2),\n",
        "$$\n",
        "where:\n",
        "$$\n",
        "\\begin{matrix}\n",
        "T_1 &=& \\frac{1}{2}m_1(l_1\\omega_1)^2 \\\\\n",
        "T_2&=&\\frac{1}{2}\\left[m_2\\left((l_1\\omega_1)^2+(l_2\\omega_2)^2 + 2l_1l_2\\omega_1\\omega_2\\cos(\\theta_2-\\theta_1)\\right)\\right] \\\\\n",
        "V_1 &=& -m_1gl_1\\cos(\\theta_1) \\\\\n",
        "V_2 &=& -m_2g\\left[l_1\\cos(\\theta_1) + l_2\\cos(\\theta_2)\\right].\n",
        "\\end{matrix}\n",
        "$$\n",
        "\n",
        "Here, we denote $\\dot q = (\\dot \\theta_1, \\dot \\theta_2)\\equiv(\\omega_1, \\omega_2)$.\n",
        "\n",
        "Let's implement this as a function in python:"
      ],
      "metadata": {
        "id": "vCrReZ05ZZ_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lagrangian(q, qt, m1, m2, l1, l2, g):\n",
        "    # We expect that q and qt are torch tensors, whose last dimention enumerates\n",
        "    # the indices 1 and 2 from the formulas above.\n",
        "    th1, th2 = q[...,0], q[...,1]\n",
        "    w1, w2 = qt[...,0], qt[...,1]\n",
        "    # th1, th2 = torch.split(q, 1, dim=-1)\n",
        "    # w1, w2 = torch.split(qt, 1, dim=-1)\n",
        "    # th1, th2 = th1.squeeze(-1), th2.squeeze(-1)\n",
        "    # w1, w2 = w1.squeeze(-1), w2.squeeze(-1)\n",
        "\n",
        "    T = 0.5 * (\n",
        "        (m1 + m2) * (l1 * w1)**2\n",
        "        + m2 * (l2 * w2)**2\n",
        "    ) + m2 * l1 * l2 * w1 * w2 * torch.cos(th2 - th1)\n",
        "    V = (\n",
        "        - (m1 + m2) * g * l1 * torch.cos(th1)\n",
        "        - m2 * g * l2 * torch.cos(th2)\n",
        "    )\n",
        "    return T - V\n",
        "\n",
        "# Let's plug our true system parameters to the Lagrangian to obtain the true\n",
        "# Lagrangian:\n",
        "true_lagrangian = partial(lagrangian, **system_parameters)"
      ],
      "metadata": {
        "id": "33_Sc95XZY9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's implement the function that will output the right hand side of our ODE (we need to provide to the ODE integrator from `torchdiffeq`):\n",
        "\n",
        "$$\\frac{d}{dt}\n",
        "\\begin{pmatrix}\n",
        "q \\\\\n",
        "\\dot q\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\dot q \\\\\n",
        "(\\nabla_{\\dot q}\\nabla_{\\dot q}^\\top L)^{-1}\\left[\\nabla_q L - (\\nabla_{\\dot q}\\nabla_q^\\top L)\\dot q\\right]\n",
        "\\end{pmatrix}.$$\n",
        "\n",
        "*N.B.: Our implementation is going to get somewhat cumbersome since we only use pure `torch` in it, which doesn't allow for batch Jacobian and Hessian calculations directly. It's probably possible to make the implementation nicer using [functorch](https://pytorch.org/functorch/stable/) (e.g., the implementation in JAX from the original [LNN tutorial](https://colab.research.google.com/drive/1CSy-xfrnTX28p1difoTA8ulYw0zytJkq) takes just 3 lines of code), though we decided to avoid that library as it's currently in beta stage.*"
      ],
      "metadata": {
        "id": "ynPGvUTYdafD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our function takes in the current `state` (the coordinates and velocities) and\n",
        "# the `lagrangian` function as a parameter and outputs the RHS of the last equation\n",
        "# above. We also add a placeholder `t` (time) parameter which is required by\n",
        "# the integrator, even though our function does not depend on time explicitly\n",
        "# (so the `t` parameter is unused).\n",
        "def ode_function(t, state, lagrangian, create_graph=True):\n",
        "    # the `state` parameter is expected to be a torch tensor whose last \n",
        "    # dimension is of size 2 and enumerates the rows of the column vector from\n",
        "    # the LHS of the last equation above.\n",
        "    q, qt = state[...,0], state[...,1]\n",
        "    # q, qt = torch.split(state, 1, dim=-1)\n",
        "    # q, qt = q.squeeze(-1), qt.squeeze(-1)\n",
        "\n",
        "    # Evaluating the Lagrangian (the output is going to be a batch of values):\n",
        "    l_value = lagrangian(q, qt)\n",
        "\n",
        "    # Calculate the gradients of the Lagrangian wrt `qt` and `q` (the output is\n",
        "    # going to be a batch of gradients):\n",
        "    (l_jac_qt,) = torch.autograd.grad(\n",
        "        l_value, qt, torch.ones(l_value.shape).to(state.device), create_graph=True\n",
        "    ) # here we force graph creation since we'll need second derivatives of this term in any case.\n",
        "    (l_jac_q,) = torch.autograd.grad(\n",
        "        l_value, q, torch.ones(l_value.shape).to(state.device), retain_graph=True, create_graph=create_graph\n",
        "    )\n",
        "\n",
        "    # Now we'll calculate the second derivatives:\n",
        "    l_second_ders = [\n",
        "        torch.autograd.grad(\n",
        "            l_jac_qt[...,i],\n",
        "            (q, qt),\n",
        "            torch.ones(l_jac_qt[...,i].shape).to(state.device),\n",
        "            retain_graph=True,\n",
        "            create_graph=create_graph,\n",
        "        ) for i in range(l_jac_qt.shape[-1])\n",
        "    ]\n",
        "    # and put them into (batches of) separate matrices\n",
        "    l_hess_qt = torch.stack(\n",
        "        [d[1] for d in l_second_ders], axis=-1\n",
        "    )\n",
        "    dl_dqdqt = torch.stack(\n",
        "        [d[0] for d in l_second_ders], axis=-2\n",
        "    ) # The `l_second_ders` list enumerates the components of dqt. Since\n",
        "      # we want the matrix multiplication (the second term in the parenthesis in\n",
        "      # the formula for acceleration) to happen over the dq indices, we put the\n",
        "      # dqt dimension into the second to last axis, which makes the last axis\n",
        "      # corresponding to dq indices.\n",
        "\n",
        "    # Calculate the invers of the Hessian (pseudo-inverse for stability):\n",
        "    l_invhess_qt = torch.linalg.pinv(l_hess_qt)\n",
        "\n",
        "    # And finally aggregate all the terms to calculate the acceleration\n",
        "    qtt = (\n",
        "        l_invhess_qt @ (l_jac_q.unsqueeze(-1) - (dl_dqdqt @ qt.unsqueeze(-1)))\n",
        "    ).squeeze(-1)\n",
        "    # and return the vector of derivatives for integration (its shape corresponds\n",
        "    # to the shape of `state`)\n",
        "    return torch.stack([qt, qtt], axis=-1)"
      ],
      "metadata": {
        "id": "-IU1Wb4SQLeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's also code the [analytical solution](https://diego.assencio.com/?index=1500c66ae7ab27bb0106467c68feebc6) to the double pendulum problem. We'll use it to generate the data (and also to check that our true Lagrangian gives the same dynamics)."
      ],
      "metadata": {
        "id": "ZJaK6kt2IG6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ode_analytical(t, state, m1, m2, l1, l2, g):\n",
        "    q, qt = state[...,0], state[...,1]\n",
        "    th1, th2 = q[...,0], q[...,1]\n",
        "    w1, w2 = qt[...,0], qt[...,1]\n",
        "\n",
        "    alpha1 = l2 / l1 * (m2 / (m1 + m2)) * torch.cos(th1 - th2)\n",
        "    alpha2 = l1 / l2 * torch.cos(th1 - th2)\n",
        "    f1 = - l2 / l1 * (m2 / (m1 + m2)) * w2**2 * torch.sin(th1 - th2) - g / l1 * torch.sin(th1)\n",
        "    f2 = l1 / l2 * w1**2 * torch.sin(th1 - th2) - g / l2 * torch.sin(th2)\n",
        "    g1 = (f1 - alpha1 * f2) / (1 - alpha1 * alpha2)\n",
        "    g2 = (f2 - alpha2 * f1) / (1 - alpha1 * alpha2)\n",
        "\n",
        "    return torch.stack([qt, torch.stack([g1, g2], axis=-1)], axis=-1)"
      ],
      "metadata": {
        "id": "uLhBY5p_Q42U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the necessary functions implemented, let's pick some random starting points and integrate them using the true Lagrangian and the analytical solution:"
      ],
      "metadata": {
        "id": "o81CPwTzJBWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = (\n",
        "    torch.randn(9, 2, 2) * 4 # A batch of 9 initial coordinates and velocities\n",
        ").clone().detach().requires_grad_(True)\n",
        "t = torch.linspace(0, 1, 101) # Advance in time for 1 second\n",
        "%time states_L = torchdiffeq.odeint(partial(ode_function, lagrangian=true_lagrangian), x0, t, method=\"rk4\")\n",
        "%time states_a = torchdiffeq.odeint(partial(ode_analytical, **system_parameters), x0, t, method=\"rk4\")"
      ],
      "metadata": {
        "id": "iC2TpTF_Q7Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the way, let's also check that we can backpropagate through the integrator:"
      ],
      "metadata": {
        "id": "gjQ3ykXTJTyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time states_L[-1, 3, 0, 0].backward() # derivative of theta_1 from the 3rd trajectory (at the last moment of time)\n",
        "x0.grad # with respect to the initial conditions"
      ],
      "metadata": {
        "id": "ZLmreTY0Rksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's check the two solutions agree (we do expect some slow divergence due to numerical errors):"
      ],
      "metadata": {
        "id": "2dv2fNFeKJjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states_L = states_L.detach().numpy()\n",
        "states_a = states_a.detach().numpy()\n",
        "\n",
        "for err in np.abs(states_L - states_a).max(axis=1).reshape(101, 4).T:\n",
        "    plt.plot(t, err);"
      ],
      "metadata": {
        "id": "HT9dkX_NSDA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like everything works as expected! Now we can generate the training and testing datasets (should run for ~2 minutes):"
      ],
      "metadata": {
        "id": "j82J7WE6KdSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 150000\n",
        "\n",
        "# Pick a single initial condition:\n",
        "x0 = torch.tensor(\n",
        "    [\n",
        "        [[3 * np.pi / 7, 0],\n",
        "         [3 * np.pi / 4, 0]]\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Advance in time for 150 seconds\n",
        "t = torch.arange(2 * N) / 1000\n",
        "states_a = torchdiffeq.odeint(\n",
        "    partial(ode_analytical, **system_parameters),\n",
        "    x0, t, atol=1e-8, rtol=1e-8\n",
        ").detach().numpy()\n",
        "\n",
        "x_train = states_a[:N].reshape(N, 2, 2)\n",
        "x_test = states_a[N:].reshape(N, 2, 2)\n",
        "\n",
        "# From each state, we'll evolve for 10 ms and set the result as the target for\n",
        "# our predictions:\n",
        "dt = 0.01\n",
        "y_train = torchdiffeq.odeint(\n",
        "    partial(ode_analytical, **system_parameters),\n",
        "    torch.tensor(x_train),\n",
        "    torch.tensor([0.0, dt]),\n",
        "    method=\"rk4\",\n",
        ")[-1].detach().numpy()\n",
        "y_test = torchdiffeq.odeint(\n",
        "    partial(ode_analytical, **system_parameters),\n",
        "    torch.tensor(x_test),\n",
        "    torch.tensor([0.0, dt]),\n",
        "    method=\"rk4\",\n",
        ")[-1].detach().numpy()"
      ],
      "metadata": {
        "id": "RH1sbY-NQC8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a visualization of our dataset:"
      ],
      "metadata": {
        "id": "ywvDT2dyMOKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is heavily borrowed from the original LNN tutorial, https://colab.research.google.com/drive/1CSy-xfrnTX28p1difoTA8ulYw0zytJkq\n",
        "def plot_dynamics(x_train, x_test):\n",
        "    x_train = x_train.transpose(0, 2, 1).reshape(-1, 4)\n",
        "    x_test = x_test.transpose(0, 2, 1).reshape(-1, 4)\n",
        "\n",
        "    def normalize_dp(state):\n",
        "        # wrap generalized coordinates to [-pi, pi]\n",
        "        return np.concatenate([(state[:,:2] + np.pi) % (2 * np.pi) - np.pi, state[:,2:]], axis=1)\n",
        "\n",
        "    # preprocess\n",
        "    train_vis = normalize_dp(x_train)\n",
        "    test_vis = normalize_dp(x_test)\n",
        "\n",
        "    vel_angle = lambda data:  (np.arctan2(data[:,3], data[:,2]) / np.pi + 1)/2\n",
        "    vel_color = lambda vangle: np.stack( [np.zeros_like(vangle), vangle, 1-vangle]).T\n",
        "    train_colors = vel_color(vel_angle(train_vis))\n",
        "    test_colors = vel_color(vel_angle(test_vis))\n",
        "\n",
        "    # plot\n",
        "    SCALE = 80 ; WIDTH = 0.006\n",
        "    plt.figure(figsize=[8,4], dpi=120)\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title(\"Train data\") ; plt.xlabel(r'$\\theta_1$') ; plt.ylabel(r'$\\theta_2$')\n",
        "    plt.quiver(*train_vis.T, color=train_colors, scale=SCALE, width=WIDTH)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title(\"Test data\") ; plt.xlabel(r'$\\theta_1$') ; plt.ylabel(r'$\\theta_2$')\n",
        "    plt.quiver(*test_vis.T, color=test_colors, scale=SCALE, width=WIDTH)\n",
        "\n",
        "    plt.tight_layout() ; plt.show()\n",
        "\n",
        "idx = np.random.choice(len(x_train), 1500)\n",
        "plot_dynamics(x_train[idx], x_test[idx])"
      ],
      "metadata": {
        "id": "myoAavAuNPww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our data is a set of 2x2 tensors of the form:\n",
        "# [[th1, w1],\n",
        "#  [th2, w2]]\n",
        "\n",
        "for i in [x_train, y_train, x_test, y_test]:\n",
        "    print(i.shape)"
      ],
      "metadata": {
        "id": "m5VkR9BqZ2NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: learning a Lagrangian of a known form"
      ],
      "metadata": {
        "id": "IheIn_88jIh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define a pytorch lighting module that only has 5 parameters ($m_1$, $m_2$, $l_1$, $l_2$ and $g$) and whose forward call plugs them into the double pendulum `lagrangian` function that we defined in the very beginning of the notebook. We then learn these 5 parameters with gradient descent."
      ],
      "metadata": {
        "id": "QMigk5tajRCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoublePendulumLagrangianModel(pl.LightningModule):\n",
        "    def __init__(self, lr, lr_decay):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters() # This is pytorch_lightning's way of tracking\n",
        "                                    # the hyperparameters (arguments of __init__).\n",
        "                                    # After this call, `lr` and `lr_decay` are\n",
        "                                    # accessible through `self.hparams.lr` and\n",
        "                                    # `self.hparams.lr_decay`.\n",
        "\n",
        "        # Let's define our learnable parameters:\n",
        "        for param_name in [\"m1\", \"m2\", \"l1\", \"l2\", \"g\"]:\n",
        "            self.register_parameter(\n",
        "                name=f\"param_{param_name}\",\n",
        "                param=torch.nn.Parameter(\n",
        "                    torch.randn(()) + 5.0, # random initialization\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.lr = self.hparams.lr\n",
        "        self.lr_decay = self.hparams.lr_decay\n",
        "\n",
        "    def forward(self, q, qt):\n",
        "        # This method should call the analytical lagrangian with the trainable\n",
        "        # parameters of our model plugged in.\n",
        "\n",
        "        return lagrangian(\n",
        "            q, qt,\n",
        "            m1=self.param_m1,\n",
        "            m2=self.param_m2,\n",
        "            l1=self.param_l1,\n",
        "            l2=self.param_l2,\n",
        "            g=self.param_g,\n",
        "        )\n",
        "\n",
        "    # We'll use the same function to calculate the loss value at both training\n",
        "    # and validation steps:\n",
        "    def _shared_loss_calculation(self, batch):\n",
        "        batch_x, batch_y = batch\n",
        "\n",
        "        # Make a single step prediction using our model as the Lagrangian function\n",
        "        pred_next_state = torchdiffeq.odeint(\n",
        "            partial(\n",
        "                ode_function,\n",
        "                lagrangian=self,\n",
        "                create_graph=self.training # this helps avoiding unnecessary\n",
        "                                           # memory allocations during validation\n",
        "            ),\n",
        "            batch_x.clone().detach().requires_grad_(True), # initial conditions\n",
        "            torch.tensor([0.0, dt]).to(self.device), # single time step\n",
        "            method=\"rk4\",\n",
        "        )[-1] # taking the state at the last moment (dt)\n",
        "        target_next_state = batch_y\n",
        "\n",
        "        # MSE loss:\n",
        "        return (\n",
        "            ((pred_next_state - target_next_state) / dt)**2 # we divide the difference by `dt`\n",
        "        ).mean()                                            # to get an estimate of the difference\n",
        "                                                            # between the derivatives\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._shared_loss_calculation(batch)\n",
        "\n",
        "        self.log(\"loss/train\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        assert not self.training\n",
        "        torch.set_grad_enabled(True) # need gradient calculation enabled in the forward pass\n",
        "        loss = self._shared_loss_calculation(batch)\n",
        "        torch.set_grad_enabled(False)\n",
        "\n",
        "        self.log(\"loss/test\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return {\n",
        "            \"optimizer\": opt,\n",
        "            \"lr_scheduler\": torch.optim.lr_scheduler.LambdaLR(\n",
        "                opt, lambda epoch: self.lr_decay**epoch # will decay learning rate exponentially\n",
        "            ),\n",
        "        }"
      ],
      "metadata": {
        "id": "_XEyUFJ0TVBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create an instance of our model and a `Trainer` object for training:"
      ],
      "metadata": {
        "id": "3bi7ZjUErsrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lagrangian = DoublePendulumLagrangianModel(lr=1e-2, lr_decay=0.96)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(\n",
        "        torch.tensor(x_train),\n",
        "        torch.tensor(y_train),\n",
        "    ),\n",
        "    batch_size=1500,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "idx_test = np.random.choice(len(x_test), 1500)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(\n",
        "        torch.tensor(x_test[idx_test]),\n",
        "        torch.tensor(y_test[idx_test]),\n",
        "    ),\n",
        "    batch_size=1500,\n",
        "    num_workers=1,\n",
        ")\n",
        "\n",
        "trainer_lagrangian = pl.Trainer(\n",
        "    gpus=1, max_epochs=15, log_every_n_steps=5,# flush_logs_every_n_steps=10,\n",
        "    callbacks=pl.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n",
        "    logger=pl.loggers.TensorBoardLogger(\n",
        "        save_dir=\"./\", name=\"lightning_logs/lagrangian\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "55HJQlKPvGfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf lightning_logs"
      ],
      "metadata": {
        "id": "PYJ40_iH0UQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run tensorboard for monitoring:"
      ],
      "metadata": {
        "id": "p1blewSvsCiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "metadata": {
        "id": "KtMbTSA0vGcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the model (keep an eye on the tensorboard window in the previous cell):"
      ],
      "metadata": {
        "id": "ckutx7O-sQEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_lagrangian.fit(model_lagrangian, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "kYYuRvfCvGYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_lagrangian_loss = model_lagrangian.trainer.logged_metrics[\"loss/test\"]\n",
        "\n",
        "assert latest_lagrangian_loss <= 1e-6, f\"Obtained loss ({latest_lagrangian_loss}) is too large.\"\n",
        "\n",
        "print(f\"Great! The loss is {latest_lagrangian_loss} <= 1e-6.\")"
      ],
      "metadata": {
        "id": "hbfSVWtLui7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: review and explain the learned parameters"
      ],
      "metadata": {
        "id": "HwIYLmUysu69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, now that we've achieved a good value of the loss, let's look at the learned parameters and compare them with the ground truth:"
      ],
      "metadata": {
        "id": "iw_7AWd9s0-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learned_parameters = dict(\n",
        "    m1=model_lagrangian.param_m1.detach().cpu().numpy(),\n",
        "    m2=model_lagrangian.param_m2.detach().cpu().numpy(),\n",
        "    l1=model_lagrangian.param_l1.detach().cpu().numpy(),\n",
        "    l2=model_lagrangian.param_l2.detach().cpu().numpy(),\n",
        "    g=model_lagrangian.param_g.detach().cpu().numpy(),\n",
        ")\n",
        "\n",
        "print(\"parameter | true value | learned value\")\n",
        "for par in [\"m1\", \"m2\", \"l1\", \"l2\", \"g\"]:\n",
        "    print(f\"  {par:2}      |   {system_parameters[par]:6.3f}   |   {learned_parameters[par]:6.3f}\")"
      ],
      "metadata": {
        "id": "JswPxcB9W0LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmmm... looks like the learned parameters are quite different from the ground truth! But the loss value is very small, i.e., we are predicting the trajectory correctly. Can you explain this?\n",
        "\n",
        "**Explain, why the learned parameters are significantly different compared to the ground truth, yet the trajectories are predicted correctly.**\n",
        "\n",
        "*Hint: look at the analytical Lagrangian and check, which transformations of it would lead to the same E-L equations. Which relations between parameters are important? Compare these relations for the ground truth and learned parameters.*"
      ],
      "metadata": {
        "id": "0qevYs0kvkW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: training an LNN"
      ],
      "metadata": {
        "id": "Baoupv0Uzl3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll assume we don't know the general form of the Lagrangian. So, we'll just model it with a fully connected neural network.\n",
        "\n",
        "Note: experiments show that LNN training is not very stable. The speed of convergence may very much depend on random initializations. In the code below we implement a trick to make the convergence more stable: we first initialize a set of `n_nets` independent networks and pretrain them all for `pretrain_epochs` epochs. After that, we select the one that gives the best validation loss and continue training it, while forgetting about the others."
      ],
      "metadata": {
        "id": "xlcqXe0Zzsn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're going to create a number of fully connected networks, it's reasonable to write a function that does it for us."
      ],
      "metadata": {
        "id": "u8HMBLU1UnBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mlp(n_in, width, depth, n_out, activation):\n",
        "    \"\"\"\n",
        "    Creates a fully connected network (an instance of torch.nn.Sequential)\n",
        "\n",
        "    Params:\n",
        "      n_in - input size\n",
        "      width - size of the hidden representations\n",
        "      depth - number of hidden representations\n",
        "      n_out - output size\n",
        "      activation - function that creates an activation layer\n",
        "    \"\"\"\n",
        "\n",
        "    # Notes:\n",
        "    #  - `activation` is a function that doesn't take any arguments, i.e. \n",
        "    #    just a call like `activation()` returns you an activation layer\n",
        "    #\n",
        "    #  - The returned object should be an instance of torch.nn.Sequential\n",
        "    #\n",
        "    #  - `depth` is the number of hidden representations. So, e.g. `depth=1`\n",
        "    #    means that there is 2 linear transformations (n_in -> width -> n_out)\n",
        "\n",
        "    layers = [\n",
        "        torch.nn.Linear(n_in, width), activation()\n",
        "    ]\n",
        "    for _ in range(depth - 1):\n",
        "        layers.append(torch.nn.Linear(width, width))\n",
        "        layers.append(activation())\n",
        "    layers.append(torch.nn.Linear(width, n_out))\n",
        "    return torch.nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def _validate_impl(n_in, width, depth, n_out, activation):\n",
        "    dummy_mlp = build_mlp(n_in, width, depth, n_out, activation)\n",
        "    num_layers = 2 * depth + 1\n",
        "\n",
        "    assert isinstance(dummy_mlp, torch.nn.Sequential), f\"Expected a torch.nn.Sequential, got {dummy_mlp.__class__.__name__} instead.\"\n",
        "    assert len(dummy_mlp) == num_layers, \"Your MLP has incorrect number of layers\"\n",
        "    for i in range(0, num_layers, 2):\n",
        "        assert isinstance(dummy_mlp[i], torch.nn.Linear), f\"Layer {i} should be a torch.nn.Linear (got {dummy_mlp[i].__class__.__name__} instead).\"\n",
        "    for i in range(1, num_layers, 2):\n",
        "        assert isinstance(dummy_mlp[i], activation), f\"Layer {i} should be a {activation.__name__} (got {dummy_mlp[i].__class__.__name__} instead).\"\n",
        "    assert dummy_mlp[0].weight.shape == (width, n_in), f\"Layer 0 should have {n_in} inputs and {width} outputs. Got {dummy_mlp[0].weight.shape[1]} inputs and {dummy_mlp[0].weight.shape[0]} outputs.\"\n",
        "    for i in range(2, num_layers - 1, 2):\n",
        "        assert dummy_mlp[i].weight.shape == (width, width), f\"Layer {i} should have {width} inputs and {width} outputs. Got {dummy_mlp[i].weight.shape[1]} inputs and {dummy_mlp[i].weight.shape[0]} outputs.\"\n",
        "    assert dummy_mlp[-1].weight.shape == (n_out, width), f\"Last layer should have {width} inputs and {n_out} outputs. Got {dummy_mlp[-1].weight.shape[1]} inputs and {dummy_mlp[-1].weight.shape[0]} outputs.\"\n",
        "\n",
        "_validate_impl(5, 7, 3, 3, torch.nn.ReLU)\n",
        "_validate_impl(16, 128, 4, 10, torch.nn.Sigmoid)\n",
        "print(\"All ok!\")"
      ],
      "metadata": {
        "id": "Ords-uBRKde9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LNN(pl.LightningModule):\n",
        "    def __init__(self, lr, lr_decay, width, n_nets, pretrain_epochs, euler_epochs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # `ModuleList` ensures that all the parameters of the children networks\n",
        "        # are registered in the parent module. Each network will be of the same\n",
        "        # architecture: two hidden layers with `width` neurons and `Softplus`\n",
        "        # activations in between. Note that we can't use ReLU activations as\n",
        "        # they will result in 0 second derivatives everywhere.\n",
        "        self.networks = torch.nn.ModuleList(\n",
        "            [build_mlp(6, width, 2, 1, torch.nn.Softplus) for _ in range(n_nets)]\n",
        "        )\n",
        "\n",
        "        self.lr = self.hparams.lr\n",
        "        self.lr_decay = self.hparams.lr_decay\n",
        "        self.n_nets = self.hparams.n_nets\n",
        "\n",
        "        # After this many epochs we'll pick the best network and train only it:\n",
        "        self.pretrain_epochs = self.hparams.pretrain_epochs\n",
        "\n",
        "        # Using 4-th order Runge-Kutta (RK4) from the beginning may be unstable, since\n",
        "        # it requires multiple evaluations of the function per step, where the\n",
        "        # evaluation locations depend on the value of the function in the first\n",
        "        # evaluation (which can, e.g., get too large). Therefore, we'll first\n",
        "        # use Euler integration scheme and only switch to RK4 after this many\n",
        "        # epochs:\n",
        "        self.euler_epochs = self.hparams.euler_epochs\n",
        "\n",
        "        self.pretraining = True\n",
        "        self.method = \"euler\"\n",
        "\n",
        "    def forward(self, q, qt, i=None):\n",
        "        # We need to tell, which of the networks to use. If `i` is not provided,\n",
        "        # pick the best one (which is decided at the end of pretraining).\n",
        "        if i is None:\n",
        "            i = self.best_id\n",
        "\n",
        "        # Since our network is fully connected, we need to put our input into\n",
        "        # a single vector. Note that `q` contains angles, which are periodic.\n",
        "        # One of the simplest ways of incorporating this periodicity into the\n",
        "        # architecture is by transforming `q` to `[sin(q), cos(q)]`:\n",
        "        x = torch.stack([torch.sin(q), torch.cos(q), qt], axis=-1)\n",
        "        return self.networks[i](x.reshape(x.shape[0], 6))\n",
        "\n",
        "    def _shared_loss_calculation(self, batch):\n",
        "        # This function is similar to the one in DoublePendulumLagrangianModel,\n",
        "        # but there's a little twist: now the behavior depends on whether we are\n",
        "        # in pretraining or training mode.\n",
        "        \n",
        "        # In the pretraining mode, we want to calculate losses for all\n",
        "        # the networks, aggregate them to minimize them simultaneously, but then\n",
        "        # also calculate the value and index of the smallest one - for monitoring\n",
        "        # and selecting the best network after pretraining.\n",
        "\n",
        "        # In the training mode, we only want to calculate the loss for the most\n",
        "        # promissing network.\n",
        "\n",
        "        batch_x, batch_y = batch\n",
        "\n",
        "        pred_next_state = torch.stack([\n",
        "            torchdiffeq.odeint(                                # This call is\n",
        "                partial(                                       # same as in the\n",
        "                    ode_function,                              # previous model\n",
        "                    lagrangian=partial(self.forward, i=i),     # (with the only\n",
        "                    create_graph=self.training                 # difference that\n",
        "                ),                                             # now we need to\n",
        "                batch_x.clone().detach().requires_grad_(True), # provide `i` to\n",
        "                torch.tensor([0.0, dt]).to(self.device),       # the forward\n",
        "                method=self.method,                            # call).\n",
        "            )[-1] for i in (\n",
        "                range(self.n_nets)       # either iterating over all the nets or\n",
        "                if self.pretraining else # picking the best one, depending on\n",
        "                [self.best_id]           # whether we're in the pretraining mode\n",
        "            )\n",
        "        ], axis=1) # <-- stack all the predictions into a single tensor\n",
        "        # the resulting shape is (batch_size, n_nets, 2, 2)\n",
        "\n",
        "        target_next_state = batch_y[:,None] # adding an axis to compare the\n",
        "                                            # target with all the predictions at\n",
        "                                            # the same time\n",
        "\n",
        "        # Averaging MSE over all the axes but the one enumerating different networks:\n",
        "        losses = (((pred_next_state - target_next_state) / dt)**2).mean(axis=(0, 2, 3))\n",
        "        # Returning a tuple:\n",
        "        return (\n",
        "            losses.sum(), # aggregated loss to minimize\n",
        "            losses.min(), # minimal loss (for monitoring during pretraining)\n",
        "            torch.argmin(losses) # id of the best network (for picking after pretraining)\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, best_loss, best_id = self._shared_loss_calculation(batch)\n",
        "\n",
        "        self.log(\"loss/train\", loss)\n",
        "        self.log(\"best_loss/train\", best_loss)\n",
        "        self.log(\"best_id/train\", best_id)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        assert not self.training\n",
        "        torch.set_grad_enabled(True)\n",
        "        loss, best_loss, best_id = self._shared_loss_calculation(batch)\n",
        "        torch.set_grad_enabled(False)\n",
        "\n",
        "        self.log(\"loss/test\", loss)\n",
        "        self.log(\"best_loss/test\", best_loss)\n",
        "        self.log(\"best_id/test\", best_id)\n",
        "        # Here we record the index of the last best network:\n",
        "        self.last_best_id = best_id.detach().cpu().numpy()\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return {\n",
        "            \"optimizer\": opt,\n",
        "            \"lr_scheduler\": torch.optim.lr_scheduler.LambdaLR(\n",
        "                opt, lambda epoch: ( # decay the learning rate only after pretraining:\n",
        "                    1.0\n",
        "                    if epoch < self.pretrain_epochs else\n",
        "                    self.lr_decay**(epoch - self.pretrain_epochs)\n",
        "                )\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        if self.trainer.current_epoch == self.pretrain_epochs:\n",
        "            print(f\"Finishing pretraining, picking id {self.last_best_id}\")\n",
        "            self.pretraining = False\n",
        "            self.best_id = self.last_best_id\n",
        "        if self.trainer.current_epoch == self.euler_epochs:\n",
        "            print(\"Switching from Euler to RK4\")\n",
        "            self.method = \"rk4\"\n"
      ],
      "metadata": {
        "id": "iM1d4crwvGjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now initialize and train an LNN model:"
      ],
      "metadata": {
        "id": "yn3_GvrR7SRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lnn = LNN(lr=1e-2, width=100, lr_decay=0.96, n_nets=32, pretrain_epochs=2, euler_epochs=20)\n",
        "\n",
        "# 40 epochs should take ~10 minutes on Colab. If you have time, feel free to\n",
        "# train for longer.\n",
        "trainer_lnn = pl.Trainer(\n",
        "    gpus=1, max_epochs=40, log_every_n_steps=5,# flush_logs_every_n_steps=10,\n",
        "    callbacks=pl.callbacks.LearningRateMonitor(logging_interval=\"epoch\"),\n",
        "    logger=pl.loggers.TensorBoardLogger(\n",
        "        save_dir=\"./\", name=\"lightning_logs/lnn\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "HPEUX6UbhLQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the training (you can use the same tensorboard window above for monitoring):"
      ],
      "metadata": {
        "id": "HCoJNLiX7XDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_lnn.fit(model_lnn, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "oC0pNOWPa_yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is trained, let's pick 9 points from the test set and evolve the system for 10 seconds - based on the analytical solution and on our model. We will then compare the resulting trajectories:"
      ],
      "metadata": {
        "id": "pE6ZAnCk7luo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = torch.tensor(x_test[np.random.choice(len(x_test), 9)], requires_grad=True)\n",
        "t = torch.linspace(0, 10, 1001)\n",
        "\n",
        "model_lnn.eval()\n",
        "%time truth = torchdiffeq.odeint(partial(ode_analytical, **system_parameters), x0, t, method=\"rk4\")\n",
        "%time preds = torchdiffeq.odeint(partial(ode_function, lagrangian=model_lnn), x0, t, method=\"rk4\")\n",
        "\n",
        "truth = truth.detach().numpy()\n",
        "preds = preds.detach().numpy()"
      ],
      "metadata": {
        "id": "-lRGPBG0nQbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_truth = \"#1060a0\"\n",
        "color_pred = \"#d08010\"\n",
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "for i_tr in range(9):\n",
        "    plt.subplot(3, 3, i_tr + 1)\n",
        "    plt.plot(t, truth[:,i_tr,0,0], color=color_truth, label=\"theta_1, truth\")\n",
        "    plt.plot(t, preds[:,i_tr,0,0], color=color_pred, label=\"theta_1, prediction\")\n",
        "    plt.plot(t, truth[:,i_tr,1,0], \"--\", color=color_truth, label=\"theta_2, truth\")\n",
        "    plt.plot(t, preds[:,i_tr,1,0], \"--\", color=color_pred, label=\"theta_2, prediction\")\n",
        "    if i_tr == 0:\n",
        "        plt.legend();\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "FgEJWYlIYhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below renders the obtained trajectories as an animation (takes about 2 minutes to run):"
      ],
      "metadata": {
        "id": "vDGslxQd8CYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PendulumVisualizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        ax,\n",
        "        trajectory,\n",
        "        params,\n",
        "        tail_len=100,\n",
        "        bone_color=\"black\",\n",
        "        bead_color=color_truth,\n",
        "        delta_z=0,\n",
        "    ):\n",
        "        self.trajectory = trajectory\n",
        "        self.tail_len = tail_len\n",
        "        self.params = params\n",
        "\n",
        "        xx, yy = self.coords_at_step(0)\n",
        "        self.line, = ax.plot(xx, yy, color=bone_color, zorder=delta_z)\n",
        "        self.c1 = plt.Circle((xx[1], yy[1]), 0.1, color=bead_color, zorder=10 + delta_z)\n",
        "        self.c2 = plt.Circle((xx[2], yy[2]), 0.1, color=bead_color, zorder=10 + delta_z)\n",
        "        self.tail = [\n",
        "            plt.Line2D([0.0, 0.0], [0.0, 0.0], zorder=delta_z - 10, color=bead_color, alpha=0.5)\n",
        "            for _ in range(self.tail_len)\n",
        "        ]\n",
        "        ax.add_patch(self.c1)\n",
        "        ax.add_patch(self.c2)\n",
        "        for t in self.tail:\n",
        "            ax.add_artist(t)\n",
        "\n",
        "    def angles_to_coords(self, q):\n",
        "        l1 = self.params[\"l1\"]\n",
        "        l2 = self.params[\"l2\"]\n",
        "\n",
        "        x1 = l1 * np.sin(q[...,0])\n",
        "        y1 = -l1 * np.cos(q[...,0])\n",
        "        x2 = x1 + l2 * np.sin(q[...,1])\n",
        "        y2 = y1 - l2 * np.cos(q[...,1])\n",
        "\n",
        "        return (np.zeros_like(x1), x1, x2), (np.zeros_like(y1), y1, y2)\n",
        "\n",
        "    def coords_at_step(self, i):\n",
        "        return self.angles_to_coords(self.trajectory[i])\n",
        "\n",
        "    def animate(self, i):\n",
        "        xx, yy = self.coords_at_step(i)\n",
        "        self.line.set_data(xx, yy)\n",
        "        self.c1.center = (xx[1], yy[1])\n",
        "        self.c2.center = (xx[2], yy[2])\n",
        "        for it in range(self.tail_len):\n",
        "            if i - it - 1 < 0:\n",
        "                self.tail[it].set_alpha(0)\n",
        "            else:\n",
        "                xx_prev = xx; yy_prev = yy\n",
        "                xx, yy = self.coords_at_step(i - it - 1)\n",
        "                self.tail[it].set_data([xx[2], xx_prev[2]], [yy[2], yy_prev[2]])\n",
        "                self.tail[it].set_alpha(1.0 - it / self.tail_len)\n",
        "        return [self.line, self.c1, self.c2] + self.tail\n",
        "\n",
        "def make_animation(trajs_A, trajs_B, n_frames=None, interval=10, label_A=\"A\", label_B=\"B\", **kwargs):\n",
        "    assert trajs_A.shape == trajs_B.shape\n",
        "\n",
        "    if n_frames is None:\n",
        "        n_frames = trajs_A.shape[1]\n",
        "    n = np.ceil(trajs_A.shape[0]**0.5).astype(int)\n",
        "\n",
        "    fig, axx = plt.subplots(n, n, figsize=(5, 5), dpi=100, squeeze=False)\n",
        "    axx = axx.reshape(-1)\n",
        "    pvs = []\n",
        "\n",
        "    first = True\n",
        "    for ax, tr_A, tr_B in zip(axx, trajs_A, trajs_B):\n",
        "        plt.sca(ax)\n",
        "        plt.xlim(-2.1, 2.1)\n",
        "        plt.ylim(-2.1, 2.1)\n",
        "        pvs.append(PendulumVisualizer(ax, tr_A, system_parameters, **kwargs))\n",
        "        pvs.append(PendulumVisualizer(\n",
        "            ax, tr_B, system_parameters, delta_z=30, bead_color=color_pred, **kwargs\n",
        "        ))\n",
        "        if first:\n",
        "            plt.legend(\n",
        "                [pvs[-2].c2, pvs[-1].c2],\n",
        "                [label_A, label_B],\n",
        "                prop={'size': 7},\n",
        "            )\n",
        "            first = False\n",
        "    for ax in axx:\n",
        "        plt.sca(ax)\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    def animate(i):\n",
        "        objs = []\n",
        "        for pv in pvs:\n",
        "            objs += pv.animate(i)\n",
        "        return objs\n",
        "\n",
        "    ani = animation.FuncAnimation(\n",
        "        fig, animate, frames=n_frames, interval=interval, blit=True\n",
        "    )\n",
        "    return fig, ani\n",
        "\n",
        "_, ani = make_animation(\n",
        "    truth[::4,...,0].transpose(1, 0, 2),\n",
        "    preds[::4,...,0].transpose(1, 0, 2),\n",
        "    interval=40, label_A=\"truth\", label_B=\"prediction\",\n",
        ")\n",
        "html5_video = ani.to_html5_video()"
      ],
      "metadata": {
        "id": "hMqIqMhgSma2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch the video:"
      ],
      "metadata": {
        "id": "k6jc7bD58N33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(html5_video)"
      ],
      "metadata": {
        "id": "HGLgGFECTAiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBBCg_MxNwM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}